{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yangyang5750858/yangyang5750858-DataScience-GenAI-Submissions/blob/main/Assignment_5/6_02_DNN_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xqQczl0FG-qtNA2_WQYuWePW9oU8irqJ)"
      ],
      "metadata": {
        "id": "E0T9_-jFXxxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.02 Dense Neural Network (with PyTorch)\n",
        "This will expand on our logistic regression example and take us through building our first neural network. If you haven't already, be sure to check (and if neccessary) switch to GPU processing by clicking Runtime > Change runtime type and selecting GPU. We can test this has worked with the following code:"
      ],
      "metadata": {
        "id": "dcEWDwlu94Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"Num GPUs Available: \", torch.cuda.device_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8cIpNbCvuQA",
        "outputId": "f0f195af-dca4-4f9a-a688-2998ed18dc03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully your code shows you have 1 GPU available! Next let's get some data. We'll start with another in-built dataset:"
      ],
      "metadata": {
        "id": "8d6FF1wK-ph8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload an in-built Python (OK semi-in-built) dataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# import the data\n",
        "data = load_diabetes()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MziWWXu-0ur",
        "outputId": "fb09463c-5739-4f6c-83b5-7c8709942a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
              "          0.01990749, -0.01764613],\n",
              "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
              "         -0.06833155, -0.09220405],\n",
              "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
              "          0.00286131, -0.02593034],\n",
              "        ...,\n",
              "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
              "         -0.04688253,  0.01549073],\n",
              "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
              "          0.04452873, -0.02593034],\n",
              "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
              "         -0.00422151,  0.00306441]]),\n",
              " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
              "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
              "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
              "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
              "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
              "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
              "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
              "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
              "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
              "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
              "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
              "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
              "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
              "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
              "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
              "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
              "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
              "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
              "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
              "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
              "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
              "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
              "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
              "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
              "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
              "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
              "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
              "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
              "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
              "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
              "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
              "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
              "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
              "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
              "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
              "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
              "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
              "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
              "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
              "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
              "        220.,  57.]),\n",
              " 'frame': None,\n",
              " 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 442\\n\\n:Number of Attributes: First 10 columns are numeric predictive values\\n\\n:Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n:Attribute Information:\\n    - age     age in years\\n    - sex\\n    - bmi     body mass index\\n    - bp      average blood pressure\\n    - s1      tc, total serum cholesterol\\n    - s2      ldl, low-density lipoproteins\\n    - s3      hdl, high-density lipoproteins\\n    - s4      tch, total cholesterol / HDL\\n    - s5      ltg, possibly log of serum triglycerides level\\n    - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\\n',\n",
              " 'feature_names': ['age',\n",
              "  'sex',\n",
              "  'bmi',\n",
              "  'bp',\n",
              "  's1',\n",
              "  's2',\n",
              "  's3',\n",
              "  's4',\n",
              "  's5',\n",
              "  's6'],\n",
              " 'data_filename': 'diabetes_data_raw.csv.gz',\n",
              " 'target_filename': 'diabetes_target.csv.gz',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are working on a regression problem, with \"structured\" data which has already been cleaned and normalised. We can skip the usual cleaning/engineering steps. However, we do need to get the data into PyTorch:"
      ],
      "metadata": {
        "id": "cZKrbx70_cIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X = torch.tensor(data.data, dtype=torch.float32)\n",
        "y = torch.tensor(data.target, dtype=torch.float32).reshape(-1, 1) # Reshape y to be a column vector"
      ],
      "metadata": {
        "id": "f9PHiljr73fI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data is stored in tensors we can do train/test splitting as before (in fact we can use sklearn as before):"
      ],
      "metadata": {
        "id": "hu8VH2_SAOoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYJN01DV8Fac",
        "outputId": "b42ccc74-ff32-4664-a1b2-30707040ff0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([353, 10]) torch.Size([353, 1])\n",
            "torch.Size([89, 10]) torch.Size([89, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set up our batches for training. As we have a nice round 400 let's go with batches of 50 (8 batches in total). We'll also seperate the features and labels:"
      ],
      "metadata": {
        "id": "LKmbZoCrJijU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
      ],
      "metadata": {
        "id": "de0uOko08d-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to build our model. We'll keep it simple ... a model with an input layer of 10 features and then 2x _Dense_ (fully connected) layers each with 5 neurons and ReLU activation. Our output layer will be size=1 given this is a regression problem and we want a single value output per prediction.\n",
        "\n",
        "This will be easier to understand if you have read through the logistic regression tutorial."
      ],
      "metadata": {
        "id": "yCCG8kKHCVnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model\n",
        "class DiabetesModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesModel, self).__init__()\n",
        "        # we'll set up the layers as a sequence using nn.Sequential\n",
        "        self.layers = nn.Sequential(\n",
        "\n",
        "            # first layer will be a linear layer that has 5x neurons\n",
        "            # (5x sets of linear regression)\n",
        "            # the layer takes the 10 features as input (i.e. 10, 5)\n",
        "            nn.Linear(10, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU activation\n",
        "\n",
        "            # second linear layer again has 5 neurons\n",
        "            # this time taking the input as the output of the last layer\n",
        "            # (which had 5x neurons)\n",
        "            nn.Linear(5, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU again\n",
        "\n",
        "            # last linear layer takes the output from the previous 5 neurons\n",
        "            # this time its a single output with no activation\n",
        "            # i.e. this is the predicitons (regression)\n",
        "            nn.Linear(5, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x) # pass the data through the layers"
      ],
      "metadata": {
        "id": "844H60hcCV3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before we need to create a model object, specify the loss (criterion) and an optimiser (which we cover next week):"
      ],
      "metadata": {
        "id": "cv4-loCz91aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = DiabetesModel()\n",
        "criterion = nn.MSELoss() # MSE loss function\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "EPx_Wy6g9uA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train the model. Again, the logistic regression tutorial (6.01) may help you undertstand this:"
      ],
      "metadata": {
        "id": "HOKfjkfW-Ish"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 100 # 100 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtMUgfwT-HGt",
        "outputId": "c9d0a98c-79e3-4d22-9a86-d4ac86fb0405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 24366.6289\n",
            "Epoch [10/100], Loss: 34809.3945\n",
            "Epoch [10/100], Loss: 23924.1211\n",
            "Epoch [10/100], Loss: 30655.3965\n",
            "Epoch [10/100], Loss: 24983.9629\n",
            "Epoch [10/100], Loss: 35090.168\n",
            "Epoch [10/100], Loss: 35043.6562\n",
            "Epoch [10/100], Loss: 4565.8086\n",
            "Epoch [20/100], Loss: 34702.625\n",
            "Epoch [20/100], Loss: 31930.5273\n",
            "Epoch [20/100], Loss: 28059.291\n",
            "Epoch [20/100], Loss: 30386.7246\n",
            "Epoch [20/100], Loss: 27855.3945\n",
            "Epoch [20/100], Loss: 28101.0195\n",
            "Epoch [20/100], Loss: 25984.6602\n",
            "Epoch [20/100], Loss: 29510.8105\n",
            "Epoch [30/100], Loss: 34797.1992\n",
            "Epoch [30/100], Loss: 29668.8203\n",
            "Epoch [30/100], Loss: 31846.2461\n",
            "Epoch [30/100], Loss: 26075.291\n",
            "Epoch [30/100], Loss: 27496.4199\n",
            "Epoch [30/100], Loss: 26569.334\n",
            "Epoch [30/100], Loss: 30520.9668\n",
            "Epoch [30/100], Loss: 23558.123\n",
            "Epoch [40/100], Loss: 29638.7422\n",
            "Epoch [40/100], Loss: 36933.4805\n",
            "Epoch [40/100], Loss: 31415.127\n",
            "Epoch [40/100], Loss: 29869.7188\n",
            "Epoch [40/100], Loss: 30518.4336\n",
            "Epoch [40/100], Loss: 24049.3086\n",
            "Epoch [40/100], Loss: 24800.1699\n",
            "Epoch [40/100], Loss: 11794.6953\n",
            "Epoch [50/100], Loss: 27411.4297\n",
            "Epoch [50/100], Loss: 24104.4629\n",
            "Epoch [50/100], Loss: 32214.3438\n",
            "Epoch [50/100], Loss: 28334.4141\n",
            "Epoch [50/100], Loss: 28669.3945\n",
            "Epoch [50/100], Loss: 35624.543\n",
            "Epoch [50/100], Loss: 28640.7852\n",
            "Epoch [50/100], Loss: 40406.0391\n",
            "Epoch [60/100], Loss: 31616.6152\n",
            "Epoch [60/100], Loss: 28343.584\n",
            "Epoch [60/100], Loss: 26658.2637\n",
            "Epoch [60/100], Loss: 27518.5664\n",
            "Epoch [60/100], Loss: 28640.0527\n",
            "Epoch [60/100], Loss: 36226.5703\n",
            "Epoch [60/100], Loss: 26020.2852\n",
            "Epoch [60/100], Loss: 30491.6523\n",
            "Epoch [70/100], Loss: 32739.0684\n",
            "Epoch [70/100], Loss: 29018.5742\n",
            "Epoch [70/100], Loss: 31401.6445\n",
            "Epoch [70/100], Loss: 29530.5703\n",
            "Epoch [70/100], Loss: 30487.2988\n",
            "Epoch [70/100], Loss: 24489.5898\n",
            "Epoch [70/100], Loss: 27903.9727\n",
            "Epoch [70/100], Loss: 10662.5938\n",
            "Epoch [80/100], Loss: 25417.4902\n",
            "Epoch [80/100], Loss: 36173.5\n",
            "Epoch [80/100], Loss: 28138.9922\n",
            "Epoch [80/100], Loss: 25793.2891\n",
            "Epoch [80/100], Loss: 33710.3438\n",
            "Epoch [80/100], Loss: 26015.6113\n",
            "Epoch [80/100], Loss: 28180.5566\n",
            "Epoch [80/100], Loss: 34906.8516\n",
            "Epoch [90/100], Loss: 31942.8984\n",
            "Epoch [90/100], Loss: 24735.9121\n",
            "Epoch [90/100], Loss: 29094.4102\n",
            "Epoch [90/100], Loss: 25871.625\n",
            "Epoch [90/100], Loss: 27034.75\n",
            "Epoch [90/100], Loss: 31772.4785\n",
            "Epoch [90/100], Loss: 31733.752\n",
            "Epoch [90/100], Loss: 43087.293\n",
            "Epoch [100/100], Loss: 31936.1836\n",
            "Epoch [100/100], Loss: 30354.0469\n",
            "Epoch [100/100], Loss: 28537.625\n",
            "Epoch [100/100], Loss: 28134.9492\n",
            "Epoch [100/100], Loss: 23605.7441\n",
            "Epoch [100/100], Loss: 24675.8301\n",
            "Epoch [100/100], Loss: 35088.5977\n",
            "Epoch [100/100], Loss: 27161.2109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see loss is significantly lower at the end than it was at the start. However, it is also bouncing around a little still which suggests the model needs more training (100 epochs is not a lot in deep learning terms). However, let's evaluate as before:"
      ],
      "metadata": {
        "id": "E72ZTKSqAODE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbuAH6p8A-Vh",
        "outputId": "52296a35-7cb3-4b20-e378-796d8bdc65b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 25403.248046875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE looks expected given training (no obvious sign of overfitting). However, we probably can get better results with tuning and more epochs.\n",
        "\n",
        "Let's run the loop again a little differently to collect the predicted values (y_hat) and actuals (y) and add them to a dataset for comparions:"
      ],
      "metadata": {
        "id": "HQ26bA08Up12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "8AYsDDSLUp_u",
        "outputId": "467aa2d4-a9f8-490a-db4f-8fb63cca7923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Predicted  Actual\n",
              "0    2.718976   219.0\n",
              "1    2.718976    70.0\n",
              "2    2.718976   202.0\n",
              "3    2.718976   230.0\n",
              "4    2.718976   111.0\n",
              "..        ...     ...\n",
              "84   2.718976   153.0\n",
              "85   2.718976    98.0\n",
              "86   2.718976    37.0\n",
              "87   2.718976    63.0\n",
              "88   2.718976   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-25efc0f9-b2e4-4c6c-8bb0-d4edd3822f72\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.718976</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.718976</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.718976</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2.718976</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.718976</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>2.718976</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>2.718976</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>2.718976</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>2.718976</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>2.718976</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-25efc0f9-b2e4-4c6c-8bb0-d4edd3822f72')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-25efc0f9-b2e4-4c6c-8bb0-d4edd3822f72 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-25efc0f9-b2e4-4c6c-8bb0-d4edd3822f72');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-96644586-ca1d-445a-b940-67e215220eed\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-96644586-ca1d-445a-b940-67e215220eed')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-96644586-ca1d-445a-b940-67e215220eed button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_59e403e5-92ca-4764-8328-4a33628ea722\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_59e403e5-92ca-4764-8328-4a33628ea722 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2.7189760208129883\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Side-by-side, they don't look great. Can you improve them?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #1\n",
        "Try increasing the number of epochs to 1,000 (when the model is fairly well trained then the results printed for each 10x epochs will be fairly stable and not change much). Does this give better results?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #2 (optional)\n",
        "Try experimenting with the architecture (number of neurons and/or number of layers). Can we reach an optimal architecture?"
      ],
      "metadata": {
        "id": "LDcM98lHbgP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 1: what we have to do now is to change the epochs, so lets copy the code of the previous sections and see the outcomes."
      ],
      "metadata": {
        "id": "0SDfhjUy7-C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000 # We need to change this row\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for inputs, targets in train_loader:\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    model.train()\n",
        "    optimiser.zero_grad()\n",
        "    outputs = model(inputs)\n",
        "    loss = criterion(outputs, targets)\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "id": "WafsBMZ58yy9",
        "outputId": "a68440da-e47c-4330-cf1a-e95eb5f3d0b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/1000], Loss: 31479.7051\n",
            "Epoch [10/1000], Loss: 23057.9766\n",
            "Epoch [10/1000], Loss: 27466.8145\n",
            "Epoch [10/1000], Loss: 26555.5742\n",
            "Epoch [10/1000], Loss: 34904.0469\n",
            "Epoch [10/1000], Loss: 28379.0176\n",
            "Epoch [10/1000], Loss: 29363.6484\n",
            "Epoch [10/1000], Loss: 31721.5703\n",
            "Epoch [20/1000], Loss: 33947.8945\n",
            "Epoch [20/1000], Loss: 25497.6035\n",
            "Epoch [20/1000], Loss: 30965.8398\n",
            "Epoch [20/1000], Loss: 24973.8145\n",
            "Epoch [20/1000], Loss: 27979.9844\n",
            "Epoch [20/1000], Loss: 27635.7793\n",
            "Epoch [20/1000], Loss: 26899.0293\n",
            "Epoch [20/1000], Loss: 71503.25\n",
            "Epoch [30/1000], Loss: 25953.0117\n",
            "Epoch [30/1000], Loss: 26720.7793\n",
            "Epoch [30/1000], Loss: 32737.7246\n",
            "Epoch [30/1000], Loss: 33376.9727\n",
            "Epoch [30/1000], Loss: 25494.5938\n",
            "Epoch [30/1000], Loss: 25483.2461\n",
            "Epoch [30/1000], Loss: 29601.709\n",
            "Epoch [30/1000], Loss: 30673.5117\n",
            "Epoch [40/1000], Loss: 29877.2539\n",
            "Epoch [40/1000], Loss: 32469.7148\n",
            "Epoch [40/1000], Loss: 30722.8242\n",
            "Epoch [40/1000], Loss: 23731.6211\n",
            "Epoch [40/1000], Loss: 31414.3691\n",
            "Epoch [40/1000], Loss: 25527.8887\n",
            "Epoch [40/1000], Loss: 25962.875\n",
            "Epoch [40/1000], Loss: 8166.8857\n",
            "Epoch [50/1000], Loss: 23700.8223\n",
            "Epoch [50/1000], Loss: 29180.334\n",
            "Epoch [50/1000], Loss: 23537.7871\n",
            "Epoch [50/1000], Loss: 31292.5117\n",
            "Epoch [50/1000], Loss: 36105.1328\n",
            "Epoch [50/1000], Loss: 30418.627\n",
            "Epoch [50/1000], Loss: 23943.1738\n",
            "Epoch [50/1000], Loss: 16001.6631\n",
            "Epoch [60/1000], Loss: 31449.6367\n",
            "Epoch [60/1000], Loss: 29548.9453\n",
            "Epoch [60/1000], Loss: 27622.3926\n",
            "Epoch [60/1000], Loss: 23128.207\n",
            "Epoch [60/1000], Loss: 30521.1797\n",
            "Epoch [60/1000], Loss: 23682.5645\n",
            "Epoch [60/1000], Loss: 30859.2246\n",
            "Epoch [60/1000], Loss: 20589.1602\n",
            "Epoch [70/1000], Loss: 28169.4668\n",
            "Epoch [70/1000], Loss: 24885.5898\n",
            "Epoch [70/1000], Loss: 25093.6523\n",
            "Epoch [70/1000], Loss: 31936.5234\n",
            "Epoch [70/1000], Loss: 26910.9023\n",
            "Epoch [70/1000], Loss: 29054.9414\n",
            "Epoch [70/1000], Loss: 29530.5723\n",
            "Epoch [70/1000], Loss: 21974.5781\n",
            "Epoch [80/1000], Loss: 32435.4434\n",
            "Epoch [80/1000], Loss: 22557.9668\n",
            "Epoch [80/1000], Loss: 25671.8867\n",
            "Epoch [80/1000], Loss: 25115.1641\n",
            "Epoch [80/1000], Loss: 33391.4492\n",
            "Epoch [80/1000], Loss: 21290.1738\n",
            "Epoch [80/1000], Loss: 33205.3711\n",
            "Epoch [80/1000], Loss: 34030.8438\n",
            "Epoch [90/1000], Loss: 25243.4824\n",
            "Epoch [90/1000], Loss: 26278.0098\n",
            "Epoch [90/1000], Loss: 25429.9023\n",
            "Epoch [90/1000], Loss: 32120.3945\n",
            "Epoch [90/1000], Loss: 26789.6387\n",
            "Epoch [90/1000], Loss: 24266.9395\n",
            "Epoch [90/1000], Loss: 32570.5645\n",
            "Epoch [90/1000], Loss: 29591.334\n",
            "Epoch [100/1000], Loss: 23842.6602\n",
            "Epoch [100/1000], Loss: 33114.0234\n",
            "Epoch [100/1000], Loss: 33644.7148\n",
            "Epoch [100/1000], Loss: 23886.1289\n",
            "Epoch [100/1000], Loss: 28681.9316\n",
            "Epoch [100/1000], Loss: 28943.5195\n",
            "Epoch [100/1000], Loss: 20159.8105\n",
            "Epoch [100/1000], Loss: 15507.2461\n",
            "Epoch [110/1000], Loss: 26003.3789\n",
            "Epoch [110/1000], Loss: 30082.5\n",
            "Epoch [110/1000], Loss: 31138.627\n",
            "Epoch [110/1000], Loss: 27218.3887\n",
            "Epoch [110/1000], Loss: 19598.0938\n",
            "Epoch [110/1000], Loss: 29556.7012\n",
            "Epoch [110/1000], Loss: 27560.3262\n",
            "Epoch [110/1000], Loss: 12352.0391\n",
            "Epoch [120/1000], Loss: 30523.7402\n",
            "Epoch [120/1000], Loss: 24393.1992\n",
            "Epoch [120/1000], Loss: 33112.5859\n",
            "Epoch [120/1000], Loss: 26497.457\n",
            "Epoch [120/1000], Loss: 20004.0879\n",
            "Epoch [120/1000], Loss: 27577.8027\n",
            "Epoch [120/1000], Loss: 27459.5488\n",
            "Epoch [120/1000], Loss: 16240.0566\n",
            "Epoch [130/1000], Loss: 25124.0391\n",
            "Epoch [130/1000], Loss: 26769.6777\n",
            "Epoch [130/1000], Loss: 24508.9551\n",
            "Epoch [130/1000], Loss: 22173.4238\n",
            "Epoch [130/1000], Loss: 27757.2734\n",
            "Epoch [130/1000], Loss: 30126.2344\n",
            "Epoch [130/1000], Loss: 32329.2148\n",
            "Epoch [130/1000], Loss: 5936.957\n",
            "Epoch [140/1000], Loss: 30198.377\n",
            "Epoch [140/1000], Loss: 31186.0137\n",
            "Epoch [140/1000], Loss: 29801.9512\n",
            "Epoch [140/1000], Loss: 22671.0039\n",
            "Epoch [140/1000], Loss: 24224.8438\n",
            "Epoch [140/1000], Loss: 21011.9688\n",
            "Epoch [140/1000], Loss: 28167.4062\n",
            "Epoch [140/1000], Loss: 8048.46\n",
            "Epoch [150/1000], Loss: 20408.4961\n",
            "Epoch [150/1000], Loss: 26856.7598\n",
            "Epoch [150/1000], Loss: 28956.8047\n",
            "Epoch [150/1000], Loss: 26027.1914\n",
            "Epoch [150/1000], Loss: 25278.2598\n",
            "Epoch [150/1000], Loss: 29120.1953\n",
            "Epoch [150/1000], Loss: 26773.4551\n",
            "Epoch [150/1000], Loss: 48650.4922\n",
            "Epoch [160/1000], Loss: 32696.9434\n",
            "Epoch [160/1000], Loss: 21739.6055\n",
            "Epoch [160/1000], Loss: 28163.1035\n",
            "Epoch [160/1000], Loss: 23989.0977\n",
            "Epoch [160/1000], Loss: 26503.3965\n",
            "Epoch [160/1000], Loss: 23827.9844\n",
            "Epoch [160/1000], Loss: 26746.9922\n",
            "Epoch [160/1000], Loss: 19945.6445\n",
            "Epoch [170/1000], Loss: 25742.3398\n",
            "Epoch [170/1000], Loss: 27332.752\n",
            "Epoch [170/1000], Loss: 22094.8574\n",
            "Epoch [170/1000], Loss: 25586.4395\n",
            "Epoch [170/1000], Loss: 29033.0996\n",
            "Epoch [170/1000], Loss: 22755.4219\n",
            "Epoch [170/1000], Loss: 28644.3125\n",
            "Epoch [170/1000], Loss: 36556.25\n",
            "Epoch [180/1000], Loss: 34262.6055\n",
            "Epoch [180/1000], Loss: 26599.1602\n",
            "Epoch [180/1000], Loss: 26925.8027\n",
            "Epoch [180/1000], Loss: 26456.8203\n",
            "Epoch [180/1000], Loss: 18002.7441\n",
            "Epoch [180/1000], Loss: 25684.9102\n",
            "Epoch [180/1000], Loss: 22982.7051\n",
            "Epoch [180/1000], Loss: 15645.9492\n",
            "Epoch [190/1000], Loss: 26690.9414\n",
            "Epoch [190/1000], Loss: 28009.8301\n",
            "Epoch [190/1000], Loss: 20822.3887\n",
            "Epoch [190/1000], Loss: 29489.9688\n",
            "Epoch [190/1000], Loss: 24614.5195\n",
            "Epoch [190/1000], Loss: 23096.1445\n",
            "Epoch [190/1000], Loss: 26363.0762\n",
            "Epoch [190/1000], Loss: 20440.4023\n",
            "Epoch [200/1000], Loss: 32515.7148\n",
            "Epoch [200/1000], Loss: 22260.0293\n",
            "Epoch [200/1000], Loss: 23241.2246\n",
            "Epoch [200/1000], Loss: 25499.4473\n",
            "Epoch [200/1000], Loss: 25193.625\n",
            "Epoch [200/1000], Loss: 22653.0352\n",
            "Epoch [200/1000], Loss: 25511.7441\n",
            "Epoch [200/1000], Loss: 31349.0996\n",
            "Epoch [210/1000], Loss: 27631.4414\n",
            "Epoch [210/1000], Loss: 24926.5938\n",
            "Epoch [210/1000], Loss: 26227.834\n",
            "Epoch [210/1000], Loss: 26579.252\n",
            "Epoch [210/1000], Loss: 26410.2891\n",
            "Epoch [210/1000], Loss: 19506.209\n",
            "Epoch [210/1000], Loss: 23384.6445\n",
            "Epoch [210/1000], Loss: 41812.7266\n",
            "Epoch [220/1000], Loss: 28118.0938\n",
            "Epoch [220/1000], Loss: 23103.5312\n",
            "Epoch [220/1000], Loss: 26755.7148\n",
            "Epoch [220/1000], Loss: 28103.0449\n",
            "Epoch [220/1000], Loss: 26612.0137\n",
            "Epoch [220/1000], Loss: 20532.8887\n",
            "Epoch [220/1000], Loss: 21990.6738\n",
            "Epoch [220/1000], Loss: 6063.9175\n",
            "Epoch [230/1000], Loss: 25849.3066\n",
            "Epoch [230/1000], Loss: 23112.832\n",
            "Epoch [230/1000], Loss: 27225.1719\n",
            "Epoch [230/1000], Loss: 22351.9648\n",
            "Epoch [230/1000], Loss: 21231.5371\n",
            "Epoch [230/1000], Loss: 30895.9219\n",
            "Epoch [230/1000], Loss: 21204.7305\n",
            "Epoch [230/1000], Loss: 35069.4531\n",
            "Epoch [240/1000], Loss: 29016.252\n",
            "Epoch [240/1000], Loss: 19208.9492\n",
            "Epoch [240/1000], Loss: 26428.7402\n",
            "Epoch [240/1000], Loss: 25231.6113\n",
            "Epoch [240/1000], Loss: 24737.4844\n",
            "Epoch [240/1000], Loss: 26000.5742\n",
            "Epoch [240/1000], Loss: 20192.1562\n",
            "Epoch [240/1000], Loss: 25048.668\n",
            "Epoch [250/1000], Loss: 14822.1172\n",
            "Epoch [250/1000], Loss: 21635.7969\n",
            "Epoch [250/1000], Loss: 19895.0801\n",
            "Epoch [250/1000], Loss: 29792.7441\n",
            "Epoch [250/1000], Loss: 28899.541\n",
            "Epoch [250/1000], Loss: 26343.3047\n",
            "Epoch [250/1000], Loss: 27826.3203\n",
            "Epoch [250/1000], Loss: 23649.8496\n",
            "Epoch [260/1000], Loss: 21166.2695\n",
            "Epoch [260/1000], Loss: 20663.0195\n",
            "Epoch [260/1000], Loss: 26441.2188\n",
            "Epoch [260/1000], Loss: 23853.4141\n",
            "Epoch [260/1000], Loss: 30170.1777\n",
            "Epoch [260/1000], Loss: 21241.6387\n",
            "Epoch [260/1000], Loss: 25342.3594\n",
            "Epoch [260/1000], Loss: 1412.6294\n",
            "Epoch [270/1000], Loss: 20784.1523\n",
            "Epoch [270/1000], Loss: 28867.7578\n",
            "Epoch [270/1000], Loss: 21536.8652\n",
            "Epoch [270/1000], Loss: 25561.8516\n",
            "Epoch [270/1000], Loss: 22015.9004\n",
            "Epoch [270/1000], Loss: 24151.7695\n",
            "Epoch [270/1000], Loss: 23907.8945\n",
            "Epoch [270/1000], Loss: 7413.5469\n",
            "Epoch [280/1000], Loss: 27929.752\n",
            "Epoch [280/1000], Loss: 24802.416\n",
            "Epoch [280/1000], Loss: 18351.8574\n",
            "Epoch [280/1000], Loss: 29921.4395\n",
            "Epoch [280/1000], Loss: 20895.9277\n",
            "Epoch [280/1000], Loss: 21814.1602\n",
            "Epoch [280/1000], Loss: 21105.4629\n",
            "Epoch [280/1000], Loss: 12398.0176\n",
            "Epoch [290/1000], Loss: 27474.8145\n",
            "Epoch [290/1000], Loss: 20176.75\n",
            "Epoch [290/1000], Loss: 24529.4785\n",
            "Epoch [290/1000], Loss: 23094.6523\n",
            "Epoch [290/1000], Loss: 21920.3828\n",
            "Epoch [290/1000], Loss: 22854.9512\n",
            "Epoch [290/1000], Loss: 22499.2578\n",
            "Epoch [290/1000], Loss: 21396.3203\n",
            "Epoch [300/1000], Loss: 21762.6953\n",
            "Epoch [300/1000], Loss: 23703.5547\n",
            "Epoch [300/1000], Loss: 18907.9043\n",
            "Epoch [300/1000], Loss: 29474.8594\n",
            "Epoch [300/1000], Loss: 21904.4688\n",
            "Epoch [300/1000], Loss: 21326.0488\n",
            "Epoch [300/1000], Loss: 24093.2148\n",
            "Epoch [300/1000], Loss: 15408.4355\n",
            "Epoch [310/1000], Loss: 25147.5391\n",
            "Epoch [310/1000], Loss: 14243.2607\n",
            "Epoch [310/1000], Loss: 21084.3965\n",
            "Epoch [310/1000], Loss: 23825.1797\n",
            "Epoch [310/1000], Loss: 23565.2441\n",
            "Epoch [310/1000], Loss: 23895.3789\n",
            "Epoch [310/1000], Loss: 27308.4473\n",
            "Epoch [310/1000], Loss: 21573.5664\n",
            "Epoch [320/1000], Loss: 20511.9473\n",
            "Epoch [320/1000], Loss: 25016.8203\n",
            "Epoch [320/1000], Loss: 23603.5469\n",
            "Epoch [320/1000], Loss: 26202.8262\n",
            "Epoch [320/1000], Loss: 21341.7461\n",
            "Epoch [320/1000], Loss: 20191.3789\n",
            "Epoch [320/1000], Loss: 20172.8066\n",
            "Epoch [320/1000], Loss: 25773.0625\n",
            "Epoch [330/1000], Loss: 24297.6895\n",
            "Epoch [330/1000], Loss: 18029.8828\n",
            "Epoch [330/1000], Loss: 28885.4609\n",
            "Epoch [330/1000], Loss: 20673.1738\n",
            "Epoch [330/1000], Loss: 22551.0996\n",
            "Epoch [330/1000], Loss: 19033.4668\n",
            "Epoch [330/1000], Loss: 22767.2715\n",
            "Epoch [330/1000], Loss: 9621.9746\n",
            "Epoch [340/1000], Loss: 29269.4551\n",
            "Epoch [340/1000], Loss: 21719.9688\n",
            "Epoch [340/1000], Loss: 20547.9824\n",
            "Epoch [340/1000], Loss: 27249.7598\n",
            "Epoch [340/1000], Loss: 18566.2227\n",
            "Epoch [340/1000], Loss: 17713.4062\n",
            "Epoch [340/1000], Loss: 18340.5176\n",
            "Epoch [340/1000], Loss: 27126.623\n",
            "Epoch [350/1000], Loss: 25010.7754\n",
            "Epoch [350/1000], Loss: 23598.9453\n",
            "Epoch [350/1000], Loss: 23241.4746\n",
            "Epoch [350/1000], Loss: 20791.9277\n",
            "Epoch [350/1000], Loss: 18013.7012\n",
            "Epoch [350/1000], Loss: 24373.0\n",
            "Epoch [350/1000], Loss: 17528.5762\n",
            "Epoch [350/1000], Loss: 11789.0078\n",
            "Epoch [360/1000], Loss: 22354.0488\n",
            "Epoch [360/1000], Loss: 21510.6387\n",
            "Epoch [360/1000], Loss: 24805.0254\n",
            "Epoch [360/1000], Loss: 22207.6113\n",
            "Epoch [360/1000], Loss: 22044.3438\n",
            "Epoch [360/1000], Loss: 15153.625\n",
            "Epoch [360/1000], Loss: 22553.9297\n",
            "Epoch [360/1000], Loss: 13573.2725\n",
            "Epoch [370/1000], Loss: 25096.4238\n",
            "Epoch [370/1000], Loss: 19915.375\n",
            "Epoch [370/1000], Loss: 20961.4805\n",
            "Epoch [370/1000], Loss: 15876.7949\n",
            "Epoch [370/1000], Loss: 23625.9492\n",
            "Epoch [370/1000], Loss: 21184.5195\n",
            "Epoch [370/1000], Loss: 21826.1445\n",
            "Epoch [370/1000], Loss: 19318.1641\n",
            "Epoch [380/1000], Loss: 20139.3047\n",
            "Epoch [380/1000], Loss: 23076.2051\n",
            "Epoch [380/1000], Loss: 15906.3223\n",
            "Epoch [380/1000], Loss: 19521.6348\n",
            "Epoch [380/1000], Loss: 21168.3223\n",
            "Epoch [380/1000], Loss: 25818.5293\n",
            "Epoch [380/1000], Loss: 20100.8711\n",
            "Epoch [380/1000], Loss: 35415.9023\n",
            "Epoch [390/1000], Loss: 19442.9473\n",
            "Epoch [390/1000], Loss: 23227.8574\n",
            "Epoch [390/1000], Loss: 21745.3398\n",
            "Epoch [390/1000], Loss: 20059.5078\n",
            "Epoch [390/1000], Loss: 22400.3887\n",
            "Epoch [390/1000], Loss: 21150.3516\n",
            "Epoch [390/1000], Loss: 16418.5176\n",
            "Epoch [390/1000], Loss: 26913.3789\n",
            "Epoch [400/1000], Loss: 23901.6699\n",
            "Epoch [400/1000], Loss: 20636.7773\n",
            "Epoch [400/1000], Loss: 25064.7188\n",
            "Epoch [400/1000], Loss: 17095.1191\n",
            "Epoch [400/1000], Loss: 19000.793\n",
            "Epoch [400/1000], Loss: 20340.3887\n",
            "Epoch [400/1000], Loss: 16838.8262\n",
            "Epoch [400/1000], Loss: 22610.7266\n",
            "Epoch [410/1000], Loss: 19023.8672\n",
            "Epoch [410/1000], Loss: 18263.584\n",
            "Epoch [410/1000], Loss: 21555.0645\n",
            "Epoch [410/1000], Loss: 19488.4785\n",
            "Epoch [410/1000], Loss: 24453.9727\n",
            "Epoch [410/1000], Loss: 21892.625\n",
            "Epoch [410/1000], Loss: 16641.6406\n",
            "Epoch [410/1000], Loss: 18143.6289\n",
            "Epoch [420/1000], Loss: 23365.627\n",
            "Epoch [420/1000], Loss: 22464.9336\n",
            "Epoch [420/1000], Loss: 15651.3096\n",
            "Epoch [420/1000], Loss: 18425.7871\n",
            "Epoch [420/1000], Loss: 19624.832\n",
            "Epoch [420/1000], Loss: 19980.6992\n",
            "Epoch [420/1000], Loss: 20968.9219\n",
            "Epoch [420/1000], Loss: 1713.4398\n",
            "Epoch [430/1000], Loss: 18253.1465\n",
            "Epoch [430/1000], Loss: 17581.7715\n",
            "Epoch [430/1000], Loss: 17494.1641\n",
            "Epoch [430/1000], Loss: 26038.4492\n",
            "Epoch [430/1000], Loss: 20865.5566\n",
            "Epoch [430/1000], Loss: 21536.9023\n",
            "Epoch [430/1000], Loss: 16619.7812\n",
            "Epoch [430/1000], Loss: 6655.2969\n",
            "Epoch [440/1000], Loss: 25666.959\n",
            "Epoch [440/1000], Loss: 18319.4375\n",
            "Epoch [440/1000], Loss: 21285.9141\n",
            "Epoch [440/1000], Loss: 17189.7246\n",
            "Epoch [440/1000], Loss: 14880.748\n",
            "Epoch [440/1000], Loss: 18395.5449\n",
            "Epoch [440/1000], Loss: 20942.0332\n",
            "Epoch [440/1000], Loss: 5116.3647\n",
            "Epoch [450/1000], Loss: 17899.168\n",
            "Epoch [450/1000], Loss: 19900.4023\n",
            "Epoch [450/1000], Loss: 19026.8945\n",
            "Epoch [450/1000], Loss: 12300.4121\n",
            "Epoch [450/1000], Loss: 21835.7441\n",
            "Epoch [450/1000], Loss: 22807.6094\n",
            "Epoch [450/1000], Loss: 20797.4199\n",
            "Epoch [450/1000], Loss: 10497.3428\n",
            "Epoch [460/1000], Loss: 18375.8828\n",
            "Epoch [460/1000], Loss: 19041.8164\n",
            "Epoch [460/1000], Loss: 23165.2754\n",
            "Epoch [460/1000], Loss: 19617.0508\n",
            "Epoch [460/1000], Loss: 16872.9414\n",
            "Epoch [460/1000], Loss: 16232.2246\n",
            "Epoch [460/1000], Loss: 19356.3516\n",
            "Epoch [460/1000], Loss: 12070.96\n",
            "Epoch [470/1000], Loss: 20388.668\n",
            "Epoch [470/1000], Loss: 19520.543\n",
            "Epoch [470/1000], Loss: 19301.8574\n",
            "Epoch [470/1000], Loss: 15931.7617\n",
            "Epoch [470/1000], Loss: 21493.3086\n",
            "Epoch [470/1000], Loss: 17681.2168\n",
            "Epoch [470/1000], Loss: 16594.4043\n",
            "Epoch [470/1000], Loss: 11493.3672\n",
            "Epoch [480/1000], Loss: 22887.4199\n",
            "Epoch [480/1000], Loss: 16389.9785\n",
            "Epoch [480/1000], Loss: 18357.2012\n",
            "Epoch [480/1000], Loss: 17758.8867\n",
            "Epoch [480/1000], Loss: 13972.5898\n",
            "Epoch [480/1000], Loss: 17921.1602\n",
            "Epoch [480/1000], Loss: 21841.6348\n",
            "Epoch [480/1000], Loss: 10821.4609\n",
            "Epoch [490/1000], Loss: 19147.4062\n",
            "Epoch [490/1000], Loss: 20906.4629\n",
            "Epoch [490/1000], Loss: 15898.2139\n",
            "Epoch [490/1000], Loss: 14777.6221\n",
            "Epoch [490/1000], Loss: 15441.6758\n",
            "Epoch [490/1000], Loss: 22170.1152\n",
            "Epoch [490/1000], Loss: 18707.418\n",
            "Epoch [490/1000], Loss: 15951.4434\n",
            "Epoch [500/1000], Loss: 19177.4727\n",
            "Epoch [500/1000], Loss: 17608.7637\n",
            "Epoch [500/1000], Loss: 18247.2422\n",
            "Epoch [500/1000], Loss: 16301.8105\n",
            "Epoch [500/1000], Loss: 17191.0801\n",
            "Epoch [500/1000], Loss: 14122.0801\n",
            "Epoch [500/1000], Loss: 22275.7363\n",
            "Epoch [500/1000], Loss: 20836.2969\n",
            "Epoch [510/1000], Loss: 14622.1924\n",
            "Epoch [510/1000], Loss: 12796.3037\n",
            "Epoch [510/1000], Loss: 19883.6426\n",
            "Epoch [510/1000], Loss: 19224.4609\n",
            "Epoch [510/1000], Loss: 18889.5371\n",
            "Epoch [510/1000], Loss: 19091.8145\n",
            "Epoch [510/1000], Loss: 18592.2129\n",
            "Epoch [510/1000], Loss: 20781.7383\n",
            "Epoch [520/1000], Loss: 21205.8438\n",
            "Epoch [520/1000], Loss: 13866.6367\n",
            "Epoch [520/1000], Loss: 17743.9062\n",
            "Epoch [520/1000], Loss: 21757.6543\n",
            "Epoch [520/1000], Loss: 11827.3936\n",
            "Epoch [520/1000], Loss: 17368.459\n",
            "Epoch [520/1000], Loss: 18266.0703\n",
            "Epoch [520/1000], Loss: 8660.6973\n",
            "Epoch [530/1000], Loss: 17651.6465\n",
            "Epoch [530/1000], Loss: 14680.8193\n",
            "Epoch [530/1000], Loss: 16538.8105\n",
            "Epoch [530/1000], Loss: 18212.2168\n",
            "Epoch [530/1000], Loss: 18837.377\n",
            "Epoch [530/1000], Loss: 17668.6113\n",
            "Epoch [530/1000], Loss: 16196.8535\n",
            "Epoch [530/1000], Loss: 16248.7363\n",
            "Epoch [540/1000], Loss: 10579.7012\n",
            "Epoch [540/1000], Loss: 15129.7275\n",
            "Epoch [540/1000], Loss: 19309.375\n",
            "Epoch [540/1000], Loss: 20069.3203\n",
            "Epoch [540/1000], Loss: 19671.252\n",
            "Epoch [540/1000], Loss: 16486.748\n",
            "Epoch [540/1000], Loss: 16547.7598\n",
            "Epoch [540/1000], Loss: 20080.2539\n",
            "Epoch [550/1000], Loss: 13719.9795\n",
            "Epoch [550/1000], Loss: 14502.6768\n",
            "Epoch [550/1000], Loss: 13827.8672\n",
            "Epoch [550/1000], Loss: 18145.8789\n",
            "Epoch [550/1000], Loss: 21887.7188\n",
            "Epoch [550/1000], Loss: 20324.9844\n",
            "Epoch [550/1000], Loss: 13524.3232\n",
            "Epoch [550/1000], Loss: 21467.6582\n",
            "Epoch [560/1000], Loss: 11809.7734\n",
            "Epoch [560/1000], Loss: 19812.2363\n",
            "Epoch [560/1000], Loss: 15307.1475\n",
            "Epoch [560/1000], Loss: 19848.1035\n",
            "Epoch [560/1000], Loss: 18467.7637\n",
            "Epoch [560/1000], Loss: 12800.0244\n",
            "Epoch [560/1000], Loss: 16286.2705\n",
            "Epoch [560/1000], Loss: 19526.7891\n",
            "Epoch [570/1000], Loss: 16224.7051\n",
            "Epoch [570/1000], Loss: 13185.0586\n",
            "Epoch [570/1000], Loss: 17509.0\n",
            "Epoch [570/1000], Loss: 16832.2754\n",
            "Epoch [570/1000], Loss: 14250.3789\n",
            "Epoch [570/1000], Loss: 16404.1914\n",
            "Epoch [570/1000], Loss: 19125.0117\n",
            "Epoch [570/1000], Loss: 3605.082\n",
            "Epoch [580/1000], Loss: 12624.7588\n",
            "Epoch [580/1000], Loss: 12605.8682\n",
            "Epoch [580/1000], Loss: 14446.2695\n",
            "Epoch [580/1000], Loss: 18075.9688\n",
            "Epoch [580/1000], Loss: 21384.8848\n",
            "Epoch [580/1000], Loss: 15329.8926\n",
            "Epoch [580/1000], Loss: 17230.9922\n",
            "Epoch [580/1000], Loss: 4854.3838\n",
            "Epoch [590/1000], Loss: 14502.8838\n",
            "Epoch [590/1000], Loss: 23508.2188\n",
            "Epoch [590/1000], Loss: 14853.0244\n",
            "Epoch [590/1000], Loss: 15359.0107\n",
            "Epoch [590/1000], Loss: 12464.377\n",
            "Epoch [590/1000], Loss: 13004.9727\n",
            "Epoch [590/1000], Loss: 15087.1543\n",
            "Epoch [590/1000], Loss: 24285.3008\n",
            "Epoch [600/1000], Loss: 14390.2168\n",
            "Epoch [600/1000], Loss: 15320.8525\n",
            "Epoch [600/1000], Loss: 13843.7695\n",
            "Epoch [600/1000], Loss: 15490.2217\n",
            "Epoch [600/1000], Loss: 17473.5195\n",
            "Epoch [600/1000], Loss: 15284.0293\n",
            "Epoch [600/1000], Loss: 16564.6953\n",
            "Epoch [600/1000], Loss: 2942.5527\n",
            "Epoch [610/1000], Loss: 15686.8467\n",
            "Epoch [610/1000], Loss: 13907.0361\n",
            "Epoch [610/1000], Loss: 13623.4443\n",
            "Epoch [610/1000], Loss: 15226.7197\n",
            "Epoch [610/1000], Loss: 17433.9414\n",
            "Epoch [610/1000], Loss: 15766.5762\n",
            "Epoch [610/1000], Loss: 14901.2998\n",
            "Epoch [610/1000], Loss: 5337.9629\n",
            "Epoch [620/1000], Loss: 16445.2402\n",
            "Epoch [620/1000], Loss: 13734.6299\n",
            "Epoch [620/1000], Loss: 14802.8398\n",
            "Epoch [620/1000], Loss: 9039.3125\n",
            "Epoch [620/1000], Loss: 18770.8477\n",
            "Epoch [620/1000], Loss: 14151.0645\n",
            "Epoch [620/1000], Loss: 18203.7578\n",
            "Epoch [620/1000], Loss: 550.1372\n",
            "Epoch [630/1000], Loss: 15689.8271\n",
            "Epoch [630/1000], Loss: 15624.3721\n",
            "Epoch [630/1000], Loss: 19732.6211\n",
            "Epoch [630/1000], Loss: 14982.416\n",
            "Epoch [630/1000], Loss: 15281.8242\n",
            "Epoch [630/1000], Loss: 11193.7832\n",
            "Epoch [630/1000], Loss: 10676.5605\n",
            "Epoch [630/1000], Loss: 6611.2808\n",
            "Epoch [640/1000], Loss: 15618.1592\n",
            "Epoch [640/1000], Loss: 11233.4639\n",
            "Epoch [640/1000], Loss: 15824.3506\n",
            "Epoch [640/1000], Loss: 11546.3994\n",
            "Epoch [640/1000], Loss: 16619.082\n",
            "Epoch [640/1000], Loss: 15501.0264\n",
            "Epoch [640/1000], Loss: 14230.2568\n",
            "Epoch [640/1000], Loss: 22930.8574\n",
            "Epoch [650/1000], Loss: 16139.7197\n",
            "Epoch [650/1000], Loss: 12312.0625\n",
            "Epoch [650/1000], Loss: 14366.9492\n",
            "Epoch [650/1000], Loss: 13294.0176\n",
            "Epoch [650/1000], Loss: 11838.833\n",
            "Epoch [650/1000], Loss: 14894.29\n",
            "Epoch [650/1000], Loss: 16316.9072\n",
            "Epoch [650/1000], Loss: 18678.4336\n",
            "Epoch [660/1000], Loss: 12756.3125\n",
            "Epoch [660/1000], Loss: 15115.9697\n",
            "Epoch [660/1000], Loss: 15408.5547\n",
            "Epoch [660/1000], Loss: 17835.2188\n",
            "Epoch [660/1000], Loss: 13256.5898\n",
            "Epoch [660/1000], Loss: 10854.6943\n",
            "Epoch [660/1000], Loss: 13290.6943\n",
            "Epoch [660/1000], Loss: 1920.1361\n",
            "Epoch [670/1000], Loss: 13235.3975\n",
            "Epoch [670/1000], Loss: 9765.8252\n",
            "Epoch [670/1000], Loss: 13867.2246\n",
            "Epoch [670/1000], Loss: 15595.3672\n",
            "Epoch [670/1000], Loss: 15443.6592\n",
            "Epoch [670/1000], Loss: 16203.5557\n",
            "Epoch [670/1000], Loss: 12165.7119\n",
            "Epoch [670/1000], Loss: 12571.4795\n",
            "Epoch [680/1000], Loss: 10964.4131\n",
            "Epoch [680/1000], Loss: 12990.8721\n",
            "Epoch [680/1000], Loss: 17133.0547\n",
            "Epoch [680/1000], Loss: 11354.96\n",
            "Epoch [680/1000], Loss: 16484.1895\n",
            "Epoch [680/1000], Loss: 16096.2148\n",
            "Epoch [680/1000], Loss: 10238.9824\n",
            "Epoch [680/1000], Loss: 2397.5156\n",
            "Epoch [690/1000], Loss: 11985.9697\n",
            "Epoch [690/1000], Loss: 12126.4473\n",
            "Epoch [690/1000], Loss: 12894.9922\n",
            "Epoch [690/1000], Loss: 11729.7246\n",
            "Epoch [690/1000], Loss: 16937.2148\n",
            "Epoch [690/1000], Loss: 13235.8945\n",
            "Epoch [690/1000], Loss: 13666.1514\n",
            "Epoch [690/1000], Loss: 21475.7949\n",
            "Epoch [700/1000], Loss: 12988.0625\n",
            "Epoch [700/1000], Loss: 11243.7051\n",
            "Epoch [700/1000], Loss: 13840.2549\n",
            "Epoch [700/1000], Loss: 12514.4385\n",
            "Epoch [700/1000], Loss: 13616.5176\n",
            "Epoch [700/1000], Loss: 12600.5801\n",
            "Epoch [700/1000], Loss: 14762.0312\n",
            "Epoch [700/1000], Loss: 12144.0664\n",
            "Epoch [710/1000], Loss: 16372.2158\n",
            "Epoch [710/1000], Loss: 13277.4863\n",
            "Epoch [710/1000], Loss: 9646.5693\n",
            "Epoch [710/1000], Loss: 15102.0957\n",
            "Epoch [710/1000], Loss: 15602.7305\n",
            "Epoch [710/1000], Loss: 11037.2275\n",
            "Epoch [710/1000], Loss: 8957.1689\n",
            "Epoch [710/1000], Loss: 12582.2061\n",
            "Epoch [720/1000], Loss: 12855.5176\n",
            "Epoch [720/1000], Loss: 9657.8418\n",
            "Epoch [720/1000], Loss: 9908.9395\n",
            "Epoch [720/1000], Loss: 14352.8438\n",
            "Epoch [720/1000], Loss: 15112.3779\n",
            "Epoch [720/1000], Loss: 16513.7402\n",
            "Epoch [720/1000], Loss: 9825.1035\n",
            "Epoch [720/1000], Loss: 16402.25\n",
            "Epoch [730/1000], Loss: 9750.9326\n",
            "Epoch [730/1000], Loss: 12312.4697\n",
            "Epoch [730/1000], Loss: 11907.7949\n",
            "Epoch [730/1000], Loss: 15456.2012\n",
            "Epoch [730/1000], Loss: 12815.5537\n",
            "Epoch [730/1000], Loss: 10899.7852\n",
            "Epoch [730/1000], Loss: 13507.7598\n",
            "Epoch [730/1000], Loss: 17061.0781\n",
            "Epoch [740/1000], Loss: 12879.1523\n",
            "Epoch [740/1000], Loss: 13769.207\n",
            "Epoch [740/1000], Loss: 9746.5322\n",
            "Epoch [740/1000], Loss: 14479.0459\n",
            "Epoch [740/1000], Loss: 8895.6963\n",
            "Epoch [740/1000], Loss: 14272.543\n",
            "Epoch [740/1000], Loss: 11741.2451\n",
            "Epoch [740/1000], Loss: 6186.7607\n",
            "Epoch [750/1000], Loss: 5923.3784\n",
            "Epoch [750/1000], Loss: 12821.8975\n",
            "Epoch [750/1000], Loss: 11008.0771\n",
            "Epoch [750/1000], Loss: 15309.8613\n",
            "Epoch [750/1000], Loss: 13045.2646\n",
            "Epoch [750/1000], Loss: 11479.9648\n",
            "Epoch [750/1000], Loss: 13588.3193\n",
            "Epoch [750/1000], Loss: 24570.8184\n",
            "Epoch [760/1000], Loss: 12864.8682\n",
            "Epoch [760/1000], Loss: 10370.4531\n",
            "Epoch [760/1000], Loss: 9718.7217\n",
            "Epoch [760/1000], Loss: 11119.7148\n",
            "Epoch [760/1000], Loss: 9846.6396\n",
            "Epoch [760/1000], Loss: 11583.9619\n",
            "Epoch [760/1000], Loss: 17135.8965\n",
            "Epoch [760/1000], Loss: 9150.4648\n",
            "Epoch [770/1000], Loss: 8249.7861\n",
            "Epoch [770/1000], Loss: 11998.1318\n",
            "Epoch [770/1000], Loss: 15949.835\n",
            "Epoch [770/1000], Loss: 7911.7173\n",
            "Epoch [770/1000], Loss: 13344.0293\n",
            "Epoch [770/1000], Loss: 14388.2227\n",
            "Epoch [770/1000], Loss: 9766.4062\n",
            "Epoch [770/1000], Loss: 2624.8335\n",
            "Epoch [780/1000], Loss: 9199.6318\n",
            "Epoch [780/1000], Loss: 13282.8447\n",
            "Epoch [780/1000], Loss: 10473.4619\n",
            "Epoch [780/1000], Loss: 13210.5225\n",
            "Epoch [780/1000], Loss: 12192.958\n",
            "Epoch [780/1000], Loss: 10838.1396\n",
            "Epoch [780/1000], Loss: 10058.2461\n",
            "Epoch [780/1000], Loss: 18722.6074\n",
            "Epoch [790/1000], Loss: 10507.0557\n",
            "Epoch [790/1000], Loss: 13199.0146\n",
            "Epoch [790/1000], Loss: 8130.5825\n",
            "Epoch [790/1000], Loss: 9575.3574\n",
            "Epoch [790/1000], Loss: 10814.835\n",
            "Epoch [790/1000], Loss: 14730.3818\n",
            "Epoch [790/1000], Loss: 11816.2461\n",
            "Epoch [790/1000], Loss: 3967.3276\n",
            "Epoch [800/1000], Loss: 12511.5127\n",
            "Epoch [800/1000], Loss: 10740.6846\n",
            "Epoch [800/1000], Loss: 12373.2119\n",
            "Epoch [800/1000], Loss: 11403.1768\n",
            "Epoch [800/1000], Loss: 6971.1152\n",
            "Epoch [800/1000], Loss: 11501.0586\n",
            "Epoch [800/1000], Loss: 11485.79\n",
            "Epoch [800/1000], Loss: 11727.3359\n",
            "Epoch [810/1000], Loss: 10227.2227\n",
            "Epoch [810/1000], Loss: 9015.167\n",
            "Epoch [810/1000], Loss: 10566.6475\n",
            "Epoch [810/1000], Loss: 9385.7334\n",
            "Epoch [810/1000], Loss: 10910.6846\n",
            "Epoch [810/1000], Loss: 13179.6826\n",
            "Epoch [810/1000], Loss: 11371.0352\n",
            "Epoch [810/1000], Loss: 28310.1777\n",
            "Epoch [820/1000], Loss: 11007.3086\n",
            "Epoch [820/1000], Loss: 10414.1338\n",
            "Epoch [820/1000], Loss: 12908.9795\n",
            "Epoch [820/1000], Loss: 12988.3535\n",
            "Epoch [820/1000], Loss: 8887.2832\n",
            "Epoch [820/1000], Loss: 8269.8848\n",
            "Epoch [820/1000], Loss: 10444.7627\n",
            "Epoch [820/1000], Loss: 1892.2351\n",
            "Epoch [830/1000], Loss: 9044.8594\n",
            "Epoch [830/1000], Loss: 11631.7471\n",
            "Epoch [830/1000], Loss: 9769.46\n",
            "Epoch [830/1000], Loss: 12457.4414\n",
            "Epoch [830/1000], Loss: 10044.9043\n",
            "Epoch [830/1000], Loss: 10062.6816\n",
            "Epoch [830/1000], Loss: 9741.96\n",
            "Epoch [830/1000], Loss: 16565.5234\n",
            "Epoch [840/1000], Loss: 9603.1289\n",
            "Epoch [840/1000], Loss: 10433.8281\n",
            "Epoch [840/1000], Loss: 12497.0\n",
            "Epoch [840/1000], Loss: 9213.1426\n",
            "Epoch [840/1000], Loss: 9553.7627\n",
            "Epoch [840/1000], Loss: 9750.7598\n",
            "Epoch [840/1000], Loss: 10073.0977\n",
            "Epoch [840/1000], Loss: 22590.0\n",
            "Epoch [850/1000], Loss: 8412.3408\n",
            "Epoch [850/1000], Loss: 8871.1055\n",
            "Epoch [850/1000], Loss: 11890.4727\n",
            "Epoch [850/1000], Loss: 10841.3096\n",
            "Epoch [850/1000], Loss: 10660.2812\n",
            "Epoch [850/1000], Loss: 9405.8369\n",
            "Epoch [850/1000], Loss: 9563.2197\n",
            "Epoch [850/1000], Loss: 27086.373\n",
            "Epoch [860/1000], Loss: 11203.5371\n",
            "Epoch [860/1000], Loss: 11175.5977\n",
            "Epoch [860/1000], Loss: 8991.4443\n",
            "Epoch [860/1000], Loss: 10327.6406\n",
            "Epoch [860/1000], Loss: 9634.1533\n",
            "Epoch [860/1000], Loss: 11563.4502\n",
            "Epoch [860/1000], Loss: 7097.1743\n",
            "Epoch [860/1000], Loss: 744.6804\n",
            "Epoch [870/1000], Loss: 9288.9092\n",
            "Epoch [870/1000], Loss: 9501.4375\n",
            "Epoch [870/1000], Loss: 11360.7744\n",
            "Epoch [870/1000], Loss: 8211.5322\n",
            "Epoch [870/1000], Loss: 6325.3486\n",
            "Epoch [870/1000], Loss: 13304.7148\n",
            "Epoch [870/1000], Loss: 10076.8252\n",
            "Epoch [870/1000], Loss: 12930.1406\n",
            "Epoch [880/1000], Loss: 9832.1377\n",
            "Epoch [880/1000], Loss: 11175.4521\n",
            "Epoch [880/1000], Loss: 9784.5986\n",
            "Epoch [880/1000], Loss: 10072.7822\n",
            "Epoch [880/1000], Loss: 7228.0454\n",
            "Epoch [880/1000], Loss: 8912.3672\n",
            "Epoch [880/1000], Loss: 10107.8105\n",
            "Epoch [880/1000], Loss: 9393.9922\n",
            "Epoch [890/1000], Loss: 7227.5674\n",
            "Epoch [890/1000], Loss: 8280.0459\n",
            "Epoch [890/1000], Loss: 11557.1201\n",
            "Epoch [890/1000], Loss: 11907.4297\n",
            "Epoch [890/1000], Loss: 10332.4902\n",
            "Epoch [890/1000], Loss: 10226.748\n",
            "Epoch [890/1000], Loss: 6976.436\n",
            "Epoch [890/1000], Loss: 346.8029\n",
            "Epoch [900/1000], Loss: 9713.7822\n",
            "Epoch [900/1000], Loss: 8007.0898\n",
            "Epoch [900/1000], Loss: 10593.0811\n",
            "Epoch [900/1000], Loss: 8546.3486\n",
            "Epoch [900/1000], Loss: 10300.6475\n",
            "Epoch [900/1000], Loss: 9263.1807\n",
            "Epoch [900/1000], Loss: 8130.9331\n",
            "Epoch [900/1000], Loss: 15330.8232\n",
            "Epoch [910/1000], Loss: 8457.8193\n",
            "Epoch [910/1000], Loss: 9442.9414\n",
            "Epoch [910/1000], Loss: 7669.6147\n",
            "Epoch [910/1000], Loss: 10481.2305\n",
            "Epoch [910/1000], Loss: 10412.7383\n",
            "Epoch [910/1000], Loss: 8458.5537\n",
            "Epoch [910/1000], Loss: 9053.0312\n",
            "Epoch [910/1000], Loss: 7034.7109\n",
            "Epoch [920/1000], Loss: 7365.5059\n",
            "Epoch [920/1000], Loss: 7793.9111\n",
            "Epoch [920/1000], Loss: 9726.3164\n",
            "Epoch [920/1000], Loss: 9532.3506\n",
            "Epoch [920/1000], Loss: 9517.25\n",
            "Epoch [920/1000], Loss: 9377.2461\n",
            "Epoch [920/1000], Loss: 9713.5918\n",
            "Epoch [920/1000], Loss: 6112.2705\n",
            "Epoch [930/1000], Loss: 9149.8877\n",
            "Epoch [930/1000], Loss: 9099.252\n",
            "Epoch [930/1000], Loss: 11106.4033\n",
            "Epoch [930/1000], Loss: 8203.4854\n",
            "Epoch [930/1000], Loss: 7486.4834\n",
            "Epoch [930/1000], Loss: 9274.374\n",
            "Epoch [930/1000], Loss: 7376.3623\n",
            "Epoch [930/1000], Loss: 11445.834\n",
            "Epoch [940/1000], Loss: 10235.4814\n",
            "Epoch [940/1000], Loss: 5325.2017\n",
            "Epoch [940/1000], Loss: 9076.7676\n",
            "Epoch [940/1000], Loss: 10648.9844\n",
            "Epoch [940/1000], Loss: 7396.1328\n",
            "Epoch [940/1000], Loss: 9707.582\n",
            "Epoch [940/1000], Loss: 8821.2266\n",
            "Epoch [940/1000], Loss: 2680.7209\n",
            "Epoch [950/1000], Loss: 8143.7266\n",
            "Epoch [950/1000], Loss: 7178.5659\n",
            "Epoch [950/1000], Loss: 7997.0684\n",
            "Epoch [950/1000], Loss: 10266.8271\n",
            "Epoch [950/1000], Loss: 9278.8564\n",
            "Epoch [950/1000], Loss: 8036.9648\n",
            "Epoch [950/1000], Loss: 9091.1104\n",
            "Epoch [950/1000], Loss: 7301.7666\n",
            "Epoch [960/1000], Loss: 9756.6055\n",
            "Epoch [960/1000], Loss: 11150.4785\n",
            "Epoch [960/1000], Loss: 5401.1372\n",
            "Epoch [960/1000], Loss: 7283.3516\n",
            "Epoch [960/1000], Loss: 9137.8818\n",
            "Epoch [960/1000], Loss: 9030.7461\n",
            "Epoch [960/1000], Loss: 7652.7168\n",
            "Epoch [960/1000], Loss: 1340.0118\n",
            "Epoch [970/1000], Loss: 9968.9873\n",
            "Epoch [970/1000], Loss: 5805.4082\n",
            "Epoch [970/1000], Loss: 7355.2847\n",
            "Epoch [970/1000], Loss: 7405.6309\n",
            "Epoch [970/1000], Loss: 9653.8994\n",
            "Epoch [970/1000], Loss: 8877.3945\n",
            "Epoch [970/1000], Loss: 9436.2334\n",
            "Epoch [970/1000], Loss: 2236.2563\n",
            "Epoch [980/1000], Loss: 9401.5752\n",
            "Epoch [980/1000], Loss: 7354.6509\n",
            "Epoch [980/1000], Loss: 5670.7373\n",
            "Epoch [980/1000], Loss: 7628.1572\n",
            "Epoch [980/1000], Loss: 6972.4287\n",
            "Epoch [980/1000], Loss: 11065.4814\n",
            "Epoch [980/1000], Loss: 9329.2168\n",
            "Epoch [980/1000], Loss: 6965.3218\n",
            "Epoch [990/1000], Loss: 5061.9307\n",
            "Epoch [990/1000], Loss: 9039.0918\n",
            "Epoch [990/1000], Loss: 8448.1006\n",
            "Epoch [990/1000], Loss: 9913.6826\n",
            "Epoch [990/1000], Loss: 6979.0024\n",
            "Epoch [990/1000], Loss: 7487.4678\n",
            "Epoch [990/1000], Loss: 9908.3154\n",
            "Epoch [990/1000], Loss: 2778.2253\n",
            "Epoch [1000/1000], Loss: 4881.6768\n",
            "Epoch [1000/1000], Loss: 9168.5176\n",
            "Epoch [1000/1000], Loss: 8604.0283\n",
            "Epoch [1000/1000], Loss: 9987.7969\n",
            "Epoch [1000/1000], Loss: 5906.6436\n",
            "Epoch [1000/1000], Loss: 8922.6982\n",
            "Epoch [1000/1000], Loss: 7533.8818\n",
            "Epoch [1000/1000], Loss: 20587.9102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets see now the average MSE\n",
        "model.eval()\n",
        "mse_values = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        mse = criterion(outputs, targets)\n",
        "        mse_values.append(mse.item())\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "id": "ik5cHgC89xGe",
        "outputId": "731d9a36-3de1-42fc-b68c-22acfd9d983f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 6372.0234375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets copare the predicted data and the actual\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "metadata": {
        "id": "3_QTdzH8-dcw",
        "outputId": "6f65efd7-e41f-41c0-9777-c18f382a1c1b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Predicted  Actual\n",
              "0   110.337082   219.0\n",
              "1   110.337082    70.0\n",
              "2   110.337082   202.0\n",
              "3   110.337082   230.0\n",
              "4   110.337082   111.0\n",
              "..         ...     ...\n",
              "84  110.337082   153.0\n",
              "85  110.337082    98.0\n",
              "86  110.337082    37.0\n",
              "87  110.337082    63.0\n",
              "88  110.337082   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a115bf8b-dc78-4419-b5ef-c245292eedaa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>110.337082</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>110.337082</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>110.337082</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>110.337082</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>110.337082</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>110.337082</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>110.337082</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>110.337082</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>110.337082</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>110.337082</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a115bf8b-dc78-4419-b5ef-c245292eedaa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a115bf8b-dc78-4419-b5ef-c245292eedaa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a115bf8b-dc78-4419-b5ef-c245292eedaa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-97cbdb78-45fa-4dd8-8942-917d4e50e6bd\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-97cbdb78-45fa-4dd8-8942-917d4e50e6bd')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-97cbdb78-45fa-4dd8-8942-917d4e50e6bd button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_ce77c282-a08b-4a3a-8376-d8d224f380b4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_ce77c282-a08b-4a3a-8376-d8d224f380b4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          110.33708190917969\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After chanching the epoch into 1000 and training the model, we can see that it improved in comparison to epoch = 100.\n",
        "\n",
        "The average MSE on the test set was approximately 6372.02, better than before but  still indicates that the model has a high prediction error.\n",
        "\n",
        "When examining the â€œPredicted vs Actualâ€ table, it can be seen that most of the predicted values are very close to 110.33, regardless of the true target value.\n",
        "\n",
        "Could be that network has not properly learned the mapping between input features and the output variable. Possible reasons include: the model being too simple, too few training epochs or too low learning rate."
      ],
      "metadata": {
        "id": "niemi3Cc-oHA"
      }
    }
  ]
}